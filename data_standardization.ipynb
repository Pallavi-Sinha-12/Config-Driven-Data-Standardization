{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import of the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7afefc04-3f9b-42f2-95f5-d88feb0f5543",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from abc import ABC, abstractmethod\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# Define spark session if not using Databricks\n",
    "spark = SparkSession.builder.appName(\"DataStandardization\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config Reader Contract (Abstract class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1595e56d-9963-481c-8d5f-da1b6d67162b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ConfigReaderContract(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def read_source_columns_schema(self)->DataFrame:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def read_new_columns_schema(self)->DataFrame:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def read_column_descriptions_metadata(self)->dict:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def read_column_sequence_order(self)->list:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation of Config Reader (JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dccacfa1-9daf-4fe3-8190-1bba6d8848a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ConfigReader(ConfigReaderContract):\n",
    "    \"\"\"\n",
    "    A class that reads and extracts information from a JSON config file.\n",
    "\n",
    "    Args:\n",
    "        config_path (str): The path to the configuration file.\n",
    "\n",
    "    Attributes:\n",
    "        config_df (DataFrame): The DataFrame containing the configuration data.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config_path):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of the ConfigReader class.\n",
    "\n",
    "        Args:\n",
    "            config_path (str): The path to the configuration file.\n",
    "\n",
    "        \"\"\"\n",
    "        self.config_df = spark.read.option(\"multiLine\", True).json(config_path)\n",
    "\n",
    "    def read_source_columns_schema(self):\n",
    "        \"\"\"\n",
    "        Reads the schema information for the source columns from the configuration file.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: The DataFrame containing the source columns schema.\n",
    "\n",
    "        \"\"\"\n",
    "        exploded_df = self.config_df.select(explode(self.config_df[\"schema\"].source_columns).alias(\"source_columns\"))\n",
    "        source_columns_schema_df = exploded_df.selectExpr(\n",
    "            \"source_columns.raw_name as raw_name\",\n",
    "            \"source_columns.standardized_name as standardized_name\",\n",
    "            \"source_columns.data_type as data_type\",\n",
    "            \"source_columns.sql_transformation as sql_transformation\"\n",
    "        )\n",
    "        return source_columns_schema_df\n",
    "\n",
    "    def read_new_columns_schema(self):\n",
    "        \"\"\"\n",
    "        Reads the schema information for the new columns from the configuration file.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: The DataFrame containing the new columns schema.\n",
    "\n",
    "        \"\"\"\n",
    "        exploded_df = self.config_df.select(explode(self.config_df[\"schema\"].new_columns).alias(\"new_columns\"))\n",
    "        new_columns_schema_df = exploded_df.selectExpr(\n",
    "            \"new_columns.name as name\",\n",
    "            \"new_columns.data_type as data_type\",\n",
    "            \"new_columns.sql_transformation as sql_transformation\"\n",
    "        )\n",
    "        return new_columns_schema_df\n",
    "\n",
    "    def read_column_descriptions_metadata(self):\n",
    "        \"\"\"\n",
    "        Reads the column descriptions metadata from the configuration file.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the column descriptions.\n",
    "\n",
    "        \"\"\"\n",
    "        metadata_df = self.config_df.select(\"metadata.column_descriptions\").alias(\"column_descriptions\")\n",
    "        descriptions_row_obj = metadata_df.first()[\"column_descriptions\"]\n",
    "        return descriptions_row_obj.asDict()\n",
    "\n",
    "    def read_column_sequence_order(self):\n",
    "        \"\"\"\n",
    "        Reads the column sequence order from the configuration file.\n",
    "\n",
    "        Returns:\n",
    "            list: A list containing the column sequence order.\n",
    "\n",
    "        \"\"\"\n",
    "        return list(self.config_df.first()[\"column_sequence_order\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Standardizer Engine Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edbca04e-b3f7-4ca7-907c-a6acdaf1cb71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class DataStandardizer:\n",
    "    \"\"\"\n",
    "    A class that performs data standardization based on configuration settings.\n",
    "\n",
    "    Args:\n",
    "        raw_dp_path (str): The path to the raw data.\n",
    "        temp_std_dp_path (str): The path to the temporary standardized data.\n",
    "        std_dp_path (str): The path to the final standardized data.\n",
    "\n",
    "    Methods:\n",
    "        create_temp_std_dp_with_source_columns(source_columns_schema_df):\n",
    "            Creates a temporary standardized data table with source columns based on the provided schema.\n",
    "        \n",
    "        add_new_columns_in_temp_std_dp(new_columns_schema_df):\n",
    "            Adds new columns to the temporary standardized data table based on the provided schema.\n",
    "        \n",
    "        update_column_descriptions_metadata(column_descriptions_dict):\n",
    "            Updates the column descriptions metadata in the temporary standardized data table.\n",
    "        \n",
    "        move_data_to_std_dp(column_sequence_order):\n",
    "            Moves the data from the temporary standardized data table to the final standardized data table.\n",
    "        \n",
    "        run(config_reader):\n",
    "            Runs the data standardization process based on the provided configuration reader.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, raw_dp_path, temp_std_dp_path, std_dp_path):\n",
    "        self.raw_dp_path = raw_dp_path\n",
    "        self.temp_std_dp_path = temp_std_dp_path\n",
    "        self.std_dp_path = std_dp_path\n",
    "\n",
    "    def create_temp_std_dp_with_source_columns(self, source_columns_schema_df):\n",
    "        source_columns_schema_df.createOrReplaceTempView(\"source_columns_config_table\")\n",
    "        select_query_sql = f\"\"\"\n",
    "            SELECT \n",
    "                concat(\n",
    "                    \"SELECT \", \n",
    "                    array_join(collect_list(select_expression), \", \"), \n",
    "                    \" FROM delta.`{self.raw_dp_path}`\"\n",
    "                ) as select_query \n",
    "            FROM (\n",
    "                SELECT \n",
    "                    CASE\n",
    "                        WHEN sql_transformation = \"\" THEN concat(\"CAST(\", concat(\"`\", raw_name, \"`\"), \" AS \", data_type, \") AS \", standardized_name)\n",
    "                        ELSE concat(\"CAST(\", sql_transformation, \" AS \", data_type, \") AS \", standardized_name)\n",
    "                    END as select_expression \n",
    "                FROM source_columns_config_table\n",
    "            )\n",
    "        \"\"\"\n",
    "        df = spark.sql(select_query_sql)\n",
    "        select_query = df.first()[\"select_query\"]\n",
    "        create_sql_query = f\"CREATE OR REPLACE TABLE delta.`{self.temp_std_dp_path}` as ( \" + select_query + \")\"\n",
    "        spark.sql(create_sql_query)\n",
    "\n",
    "\n",
    "    def add_new_columns_in_temp_std_dp(self, new_columns_schema_df):\n",
    "        new_columns_schema_df_rows = new_columns_schema_df.collect()        \n",
    "        for row in new_columns_schema_df_rows:\n",
    "            add_new_columns_sql = f\"ALTER TABLE delta.`{self.temp_std_dp_path}` ADD COLUMN {row['name']} {row['data_type']}\"   \n",
    "            sql_transformation = row[\"sql_transformation\"].replace(\"{temp_std_dp_path}\", self.temp_std_dp_path)  \n",
    "            spark.sql(add_new_columns_sql)  \n",
    "            spark.sql(sql_transformation)      \n",
    "    \n",
    "    def update_column_descriptions_metadata(self, column_descriptions_dict):\n",
    "        for column_name,description in column_descriptions_dict.items():\n",
    "            column_description_update_sql = f\"ALTER TABLE delta.`{self.temp_std_dp_path}` CHANGE COLUMN {column_name} COMMENT '{description}';\"\n",
    "            spark.sql(column_description_update_sql)\n",
    "        \n",
    "    def move_data_to_std_dp(self, column_sequence_order):\n",
    "        temp_std_df = spark.read.format(\"delta\").load(self.temp_std_dp_path)\n",
    "        temp_std_df = temp_std_df.select(column_sequence_order)\n",
    "        temp_std_df.write.option(\"mergeSchema\", \"true\").format(\"delta\").mode(\"overwrite\").save(self.std_dp_path)\n",
    "\n",
    "    def run(self, config_reader):\n",
    "        print(\"Raw df : \")\n",
    "        raw_df = spark.read.format(\"delta\").load(self.raw_dp_path)\n",
    "        display(raw_df)\n",
    "\n",
    "        source_columns_schema_df = config_reader.read_source_columns_schema()\n",
    "        self.create_temp_std_dp_with_source_columns(source_columns_schema_df)\n",
    "\n",
    "        new_columns_schema_df = config_reader.read_new_columns_schema()\n",
    "        self.add_new_columns_in_temp_std_dp(new_columns_schema_df)\n",
    "\n",
    "        column_descriptions_dict = config_reader.read_column_descriptions_metadata()\n",
    "        self.update_column_descriptions_metadata(column_descriptions_dict)\n",
    "\n",
    "        column_sequence_order = config_reader.read_column_sequence_order()\n",
    "        self.move_data_to_std_dp(column_sequence_order)\n",
    "\n",
    "        print(\"Standardized df : \")\n",
    "        std_df = spark.read.format(\"delta\").load(self.std_dp_path)\n",
    "        display(std_df)\n",
    "\n",
    "        print(\"Schema information for Standardized df : \")\n",
    "        std_df.printSchema()    \n",
    "        display(spark.sql(f\"DESCRIBE TABLE delta.`{self.std_dp_path}`\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4367ba9d-bf80-42a4-ba96-73114be370ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Replace the below paths with your own desired paths\n",
    "raw_dp_path = \"dbfs:/FileStore/project/supplier\"\n",
    "std_dp_path = \"dbfs:/FileStore/project/Product_Supplier\"\n",
    "temp_std_dp_path = \"dbfs:/FileStore/project/Product_Supplier_temp\"\n",
    "config_path = \"dbfs:/FileStore/project/supplier_config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f392c362-f0f5-4520-9ad1-115ac47784cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "config_reader = ConfigReader(config_path)\n",
    "data_standardizer = DataStandardizer(\n",
    "    raw_dp_path=raw_dp_path,\n",
    "    temp_std_dp_path=temp_std_dp_path,\n",
    "    std_dp_path=std_dp_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02fbf70d-ec03-424d-8556-4bee586d3036",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw df : \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>sup_id</th><th>name</th><th>price</th><th>prod_name</th><th>quantity</th><th>email</th></tr></thead><tbody><tr><td>9999</td><td>john</td><td>10</td><td>ball</td><td>100</td><td>john@email.com</td></tr><tr><td>9876</td><td>mary</td><td>20</td><td>kite</td><td>200</td><td>mary@email.com</td></tr><tr><td>8765</td><td>ram</td><td>330</td><td>bat</td><td>300</td><td>ram@email.com</td></tr><tr><td>7654</td><td>rahim</td><td>400</td><td>football</td><td>40</td><td>rahim@email.com</td></tr><tr><td>6543</td><td>sita</td><td>560</td><td>badminton</td><td>500</td><td>sita@email.com</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "9999",
         "john",
         "10",
         "ball",
         "100",
         "john@email.com"
        ],
        [
         "9876",
         "mary",
         "20",
         "kite",
         "200",
         "mary@email.com"
        ],
        [
         "8765",
         "ram",
         "330",
         "bat",
         "300",
         "ram@email.com"
        ],
        [
         "7654",
         "rahim",
         "400",
         "football",
         "40",
         "rahim@email.com"
        ],
        [
         "6543",
         "sita",
         "560",
         "badminton",
         "500",
         "sita@email.com"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "sup_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "prod_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "quantity",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "email",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized df : \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Supplier_ID</th><th>Supplier_Name</th><th>Product_ID</th><th>Product_Name</th><th>Purchase_Price</th><th>Purchase_Quantity</th><th>Total_Cost</th></tr></thead><tbody><tr><td>SUP-9999</td><td>john</td><td>PROD-05</td><td>ball</td><td>10</td><td>100</td><td>1000</td></tr><tr><td>SUP-9876</td><td>mary</td><td>PROD-06</td><td>kite</td><td>20</td><td>200</td><td>4000</td></tr><tr><td>SUP-8765</td><td>ram</td><td>PROD-04</td><td>bat</td><td>330</td><td>300</td><td>99000</td></tr><tr><td>SUP-7654</td><td>rahim</td><td>PROD-01</td><td>football</td><td>400</td><td>40</td><td>16000</td></tr><tr><td>SUP-6543</td><td>sita</td><td>PROD-03</td><td>badminton</td><td>560</td><td>500</td><td>280000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "SUP-9999",
         "john",
         "PROD-05",
         "ball",
         10,
         100,
         1000
        ],
        [
         "SUP-9876",
         "mary",
         "PROD-06",
         "kite",
         20,
         200,
         4000
        ],
        [
         "SUP-8765",
         "ram",
         "PROD-04",
         "bat",
         330,
         300,
         99000
        ],
        [
         "SUP-7654",
         "rahim",
         "PROD-01",
         "football",
         400,
         40,
         16000
        ],
        [
         "SUP-6543",
         "sita",
         "PROD-03",
         "badminton",
         560,
         500,
         280000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"comment\":\"Unique identifier for the supplier of a product\"}",
         "name": "Supplier_ID",
         "type": "\"string\""
        },
        {
         "metadata": "{\"comment\":\"Name of the supplier\"}",
         "name": "Supplier_Name",
         "type": "\"string\""
        },
        {
         "metadata": "{\"comment\":\"Unique identifier for the product\"}",
         "name": "Product_ID",
         "type": "\"string\""
        },
        {
         "metadata": "{\"comment\":\"Name of the product\"}",
         "name": "Product_Name",
         "type": "\"string\""
        },
        {
         "metadata": "{\"comment\":\"Price at which the supplier sells the product\"}",
         "name": "Purchase_Price",
         "type": "\"integer\""
        },
        {
         "metadata": "{\"comment\":\"Quantity of the product available with the supplier\"}",
         "name": "Purchase_Quantity",
         "type": "\"integer\""
        },
        {
         "metadata": "{\"comment\":\"Total amount spent on purchasing a specific quantity of items at the given purchase price.\"}",
         "name": "Total_Cost",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema information for Standardized df : \n",
      "root\n",
      " |-- Supplier_ID: string (nullable = true)\n",
      " |-- Supplier_Name: string (nullable = true)\n",
      " |-- Product_ID: string (nullable = true)\n",
      " |-- Product_Name: string (nullable = true)\n",
      " |-- Purchase_Price: integer (nullable = true)\n",
      " |-- Purchase_Quantity: integer (nullable = true)\n",
      " |-- Total_Cost: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>col_name</th><th>data_type</th><th>comment</th></tr></thead><tbody><tr><td>Supplier_ID</td><td>string</td><td>Unique identifier for the supplier of a product</td></tr><tr><td>Supplier_Name</td><td>string</td><td>Name of the supplier</td></tr><tr><td>Product_ID</td><td>string</td><td>Unique identifier for the product</td></tr><tr><td>Product_Name</td><td>string</td><td>Name of the product</td></tr><tr><td>Purchase_Price</td><td>int</td><td>Price at which the supplier sells the product</td></tr><tr><td>Purchase_Quantity</td><td>int</td><td>Quantity of the product available with the supplier</td></tr><tr><td>Total_Cost</td><td>int</td><td>Total amount spent on purchasing a specific quantity of items at the given purchase price.</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Supplier_ID",
         "string",
         "Unique identifier for the supplier of a product"
        ],
        [
         "Supplier_Name",
         "string",
         "Name of the supplier"
        ],
        [
         "Product_ID",
         "string",
         "Unique identifier for the product"
        ],
        [
         "Product_Name",
         "string",
         "Name of the product"
        ],
        [
         "Purchase_Price",
         "int",
         "Price at which the supplier sells the product"
        ],
        [
         "Purchase_Quantity",
         "int",
         "Quantity of the product available with the supplier"
        ],
        [
         "Total_Cost",
         "int",
         "Total amount spent on purchasing a specific quantity of items at the given purchase price."
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"comment\":\"name of the column\"}",
         "name": "col_name",
         "type": "\"string\""
        },
        {
         "metadata": "{\"comment\":\"data type of the column\"}",
         "name": "data_type",
         "type": "\"string\""
        },
        {
         "metadata": "{\"comment\":\"comment of the column\"}",
         "name": "comment",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_standardizer.run(config_reader)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data_standardization",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
